{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Woordbetekenissen door de ogen van de computer**"
      ],
      "metadata": {
        "id": "4ZFU1XwPmBtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vooraleer in de notebook te vliegen is het belangrijk om onderstaande codecel te activeren (klik daarvoor op de \"play\"-knop links in de cel) en daarna bovenaan in de toolbar op `Runtime > Sessie opnieuw starten` te klikken."
      ],
      "metadata": {
        "id": "pH7f8vgGLw5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "GaLS1h47Mq-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inleiding**\n",
        "In menselijke taal worden letters gebruikt om woorden te vormen. Die woorden worden op hun beurt gebruikt om naar objecten of concepten te verwijzen. Door woorden (en hun achterliggende objecten/concepten) te combineren in zinnen, kunnen mensen communiceren met elkaar. In deze notebook zullen we uitleggen hoe computers kijken naar letters, woorden en objecten/concepten. Zien computers hetzelfde wat mensen zien? Kunnen ze combinaties van letters herkennen als woorden? En hoe linken ze woorden aan hun betekenis(sen)?"
      ],
      "metadata": {
        "id": "X6Ovce9LLt-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Van woorden naar vectoren**\n",
        "Wat betreft de eerste vraag kunnen we alvast kort zijn: neen, computers zien niet wat wij zien. Waar mensen letters zien, staan er voor de computer nulletjes en eentjes (of *bytes*, om het in computertermen te zeggen). Waar mensen letters samenvoegen in woorden, ziet de computer reeksen van *bytes*. Waar mensen woorden combineren in de vorm van zinnen, zwemt de computer in een zee van *byte*-reeksen gescheiden door spaties. Met andere woorden, als we willen dat de computer taal begrijpt, moeten we een manier zoeken om betekenisrelaties te vinden in die *bytes*. Een eerste stap hierbij is dat we woorden proberen om te zetten in unieke cijferwaardes, in het vakjargon \"statische woordvectoren\" genoemd.\n",
        "\n",
        "### **Statische woordvectoren**\n",
        "Het startpunt voor statische woordvectoren wordt gevormd door een van de kernprincipes uit de distributionele semantiek: \"you shall know a word by the company it keeps\" (Firth, 1957). Stel dat we een grote verzameling teksten (bv. alle Wikipediapagina's in een bepaalde taal) aan de computer geven, dan is het idee dat we de computer een model laten trainen dat bijhoudt welke *byte*-reeksen (woorden, dus) vaak in elkaars buurt voorkomen en deze reeksen een gelijkaardige cijferwaarde toekent.\n",
        "\n",
        "In de codecellen hieronder maken we dit alles een tikkeltje concreter. Eerst laden we een model waarin statische woordvectoren opgeslagen zijn (klik daarvoor op de \"play\"-knop van de eerste cel). Ter info: de grote verzameling teksten die aan de computer gegeven werd om dit model te trainen bestaat uit twee miljard tweets. Vervolgens gaan we over naar de tweede cel: in `woord_1` en `woord_2` definiëren we twee woorden die we daarna opzoeken in ons model. De vector van de twee woorden printen we op het scherm (activeer de code door op de \"play\"-knop van de tweede cel te klikken). Zoals je zal kunnen zien, zijn woordvectoren simpelweg een reeks van cijfers (of \"dimensies\", in iets technischere termen). Aangezien het model hoofdzakelijk op Engelse data getraind is, gebruiken we de woorden *fork* en *knife* in ons voorbeeld. Je kan de code gerust uitproberen met je eigen (Nederlandse) woorden, maar hou er rekening mee dat de resultaten niet altijd even accuraat zullen zijn, zeker aangezien we voor deze notebook slechts met een klein model werken."
      ],
      "metadata": {
        "id": "0BSga7-A210S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WEETJE**: Woordvectoren worden vaak ook *word embeddings* genoemd, aangezien ze proberen te vatten hoe woorden *embedded* (\"ingebed\") zijn in de contexten waarin ze voorkomen."
      ],
      "metadata": {
        "id": "V5siuK3qLtDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OPMERKING**: Het laden van het model met de statische vectoren kan eventjes duren."
      ],
      "metadata": {
        "id": "i6M3WyRWLsyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# model laden dat woorden heeft omgezet in statische vectoren\n",
        "glove_vectoren = api.load(\"glove-twitter-25\")"
      ],
      "metadata": {
        "id": "zx_40V9w9I_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# twee woorden definiëren waarvan we de vector willen weten\n",
        "woord_1 = \"fork\"\n",
        "woord_2 = \"knife\"\n",
        "\n",
        "# model raadplegen om vectoren te weten te komen\n",
        "vector_1 = glove_vectoren[woord_1]\n",
        "vector_2 = glove_vectoren[woord_2]\n",
        "\n",
        "# vectoren op het scherm printen\n",
        "print(f\"De woordvector van '{woord_1}' is: {vector_1}\")\n",
        "print(f\"De woordvector van '{woord_2}' is: {vector_2}\")"
      ],
      "metadata": {
        "id": "g-efTgPCmy54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OPMERKING**: Als je waarden in een codecel wijzigt, moet je opnieuw op de \"play\"-knop van alle cellen klikken om er zeker van te zijn dat deze wijzingen meegenomen worden in de volledige notebook."
      ],
      "metadata": {
        "id": "WSBWSqnKOKHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Als we beide vectoren vergelijken door te kijken naar de overeenkomstige waardes in elke dimensie, dan kunnen we vaststellen dat - met hier en daar een korreltje zout - deze waardes grotendeels in elkaars buurt liggen. Met andere woorden, de computer lijkt te begrijpen dat *fork* bepaalde zaken gemeen heeft met *knife*. Maar hoeveel precies hebben ze met elkaar gemeen? En hoe moeten we deze mate van overeenkomst interpreteren ten opzichte van andere woorden? Om deze vragen te beantwoorden, kunnen we eerst en vooral gebruik maken van **wiskundige formules die het aantal dimensies van de vector reduceren** naar een lager aantal.\n",
        "\n",
        "Hierdoor kunnen we vectoren plotten in een standaardgrafiek met een x-as en y-as (twee dimensies, dus), wat de interpretatie van de vectoren gevoelig vereenvoudigt. In onderstaand voorbeeld vertrekken we van de volgende lijst items: `woord_1` en `woord_2` (zie boven), met daarnaast nog vier nieuwe woorden (`woord_3` - `woord_6`). We halen de vectoren van deze woorden op in ons model en herleiden hun oorspronkelijke aantal van 25 dimensies naar 2 dimensies via een wiskundige ingreep uit de lineaire algebra, *principal component analysis* genaamd (afgekort als \"PCA\"). Vervolgens printen we de effectieve waarden van de gereduceerde vectoren op het scherm en plotten we de drie woorden in een tweedimensionale grafiek. Druk op de \"play\"-knop linksboven in de cel om de code de activeren.\n",
        "\n",
        "De grafiek die we te zien krijgen als resultaat schetst best een accuraat beeld: *fork*, *knife*, en *spoon* staan eerst en vooral mooi dicht bij elkaar. Ook *plate* ligt niet ver uit de buurt, maar lijkt zich toch in een lichtjes andere zone van de grafiek te situeren. *Radio* en *screen* liggen dan weer heel wat verderaf. Ook dit klopt intuïtief volledig, aangezien het twee woorden zijn die bitter weinig te maken hebben met het semantische veld rond borden en bestek."
      ],
      "metadata": {
        "id": "A8JIgrlSyJAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OPMERKING**: Experimenteer gerust opnieuw met zelfgekozen woorden."
      ],
      "metadata": {
        "id": "zAU9g901IU94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# extra woorden definiëren\n",
        "woord_3 = \"spoon\"\n",
        "woord_4 = \"plate\"\n",
        "woord_5 = \"radio\"\n",
        "woord_6 = \"screen\"\n",
        "\n",
        "# model raadplegen om vectoren van deze woorden te verkrijgen\n",
        "l_woorden = [woord_1, woord_2, woord_3, woord_4, woord_5, woord_6]\n",
        "l_vectoren = [glove_vectoren[woord] for woord in l_woorden]\n",
        "\n",
        "# vectoren reduceren naar twee dimensies en cijferwaardes op het scherm printen\n",
        "l_vectoren_gereduceerd = PCA(n_components=2).fit_transform(l_vectoren)\n",
        "\n",
        "for idx, woord in enumerate(l_woorden):\n",
        "  print(f\"Gereduceerde vector van '{woord}': {l_vectoren_gereduceerd[idx]}\")\n",
        "\n",
        "print(\"\\n-----------\\n\")\n",
        "\n",
        "# vectoren plotten in tweedimensionale grafiek\n",
        "df_vectoren = pd.DataFrame(l_vectoren_gereduceerd)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
        "\n",
        "def woorden_plotten_in_2d(df: pd.DataFrame, l_labels: list, *, afstand_label: float = 0.5) -> None:\n",
        "    scatterplot = sns.scatterplot(data=df, x=0, y=1)\n",
        "    sns.despine()\n",
        "    scatterplot.spines[\"bottom\"].set_linewidth(2)\n",
        "    scatterplot.spines[\"left\"].set_linewidth(2)\n",
        "    scatterplot.set(xticklabels=[], yticklabels=[])\n",
        "    scatterplot.set_xlabel(\"x\")\n",
        "    scatterplot.set_ylabel(\"y\")\n",
        "    scatterplot.tick_params(bottom=False, left=False)\n",
        "\n",
        "    for idx, label in enumerate(l_labels):\n",
        "        plt.text((df.loc[idx][0] + afstand_label), (df.loc[idx][1] + afstand_label), label)\n",
        "\n",
        "woorden_plotten_in_2d(df_vectoren, l_woorden, afstand_label=0.05)"
      ],
      "metadata": {
        "id": "Hj-dqXprUUMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Een tweede manier om te interpretatie van woordvectoren voor ons als mensen te vergemakkelijken zijn **wiskundige formules die de waardes van twee vectoren vergelijken** en omzetten in één enkele numerieke score. Een van die formules is de *cosine similarity*: deze maat geeft als output een score tussen -1 en 1 (hoe dichter bij 1, hoe groter de overeenkomst). In de codecel hieronder berekenen we de *cosine similarity* tussen drie woorden: `woord_1`, `woord_2`, en `woord_5` (zie boven). Als je niets aan de code veranderd hebt, vergelijken we dus *fork*, *knife*, en *radio*. De resultaten geven heel duidelijk aan dat *fork* en *knife* grote gelijkenissen vertonen (score van 0.9), wat niet kan gezegd worden voor *fork* versus *radio* (score van 0.18) en *knife* versus *radio* (0.35). Deze resultaten bevestigen dus de conclusie die we zonet al trokken op basis van de grafische representatie op basis van de tweedimensionale vectoren."
      ],
      "metadata": {
        "id": "Iv0iCB0gTh-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cosine similarity berekenen\n",
        "cos_sim_1_2 = glove_vectoren.similarity(woord_1, woord_2)\n",
        "cos_sim_1_5 = glove_vectoren.similarity(woord_1, woord_5)\n",
        "cos_sim_2_5 = glove_vectoren.similarity(woord_2, woord_5)\n",
        "\n",
        "# scores printen op het scherm\n",
        "print(f\"De cosine similarity tussen de woorden '{woord_1}' en '{woord_2}' bedraagt {round(float(cos_sim_1_2), 2)}.\")\n",
        "print(f\"De cosine similarity tussen de woorden '{woord_1}' en '{woord_5}' bedraagt {round(float(cos_sim_1_5), 2)}.\")\n",
        "print(f\"De cosine similarity tussen de woorden '{woord_2}' en '{woord_5}' bedraagt {round(float(cos_sim_2_5), 2)}.\")"
      ],
      "metadata": {
        "id": "5R0a2qRAmGcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Als tussentijdse conclusie kunnen we stellen dat statische woordvectoren de computer in staat stellen om betekenisvolle verbanden te leggen tussen woorden en de contexten waarin ze voorkomen. Met enige zin voor overdrijving kunnen we zelfs stellen dat computers op deze manier ook een soort van \"wereldkennis\" vergaren. Ter illustratie hiervan berekenen we via onderstaande codecel de mate van gelijkenis tussen het woord *tree* (boom) en een reeks van andere woorden: *leaf* (blad), *forest* (bos), *nature* (natuur), en *technology* (technologie).\n",
        "\n",
        "De scores geven aan dat *boom* en *blad* erg aan elkaar gelinkt zijn (score van 0.85). Ook *boom* en *bos* vertonen een grote gelijkenis (0.81), zij het iets minder groot dan *boom* en *blad*. Aangezien bomen onlosmakelijk verbonden zijn met bladeren (met naaldbomen als de uitzondering die de regel bevestigen) maar niet per se altijd in een bos voorkomen, is de hogere waarde voor *boom* versus *blad* best logisch. Hetzelfde geldt voor *boom* en *natuur*: de score van 0.71 geeft aan dat er een duidelijke link is, maar die link is minder sterk dan met *blad* of *bos*. Tot slot blijkt uit de score van 0.52 tussen *boom* en *technologie* dat de computer \"begrijpt\" dat technologie beduidend minder te maken heeft met een boom dan het geval is voor een blad, een bos, of de natuur."
      ],
      "metadata": {
        "id": "UuiudQ2WFy2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TIP**: Meer gedetailleerde informatie rond statische woordvectoren is te vinden in het werk van [Mikolov et al. (2013)](https://arxiv.org/pdf/1301.3781), een referentiepaper waarin het invloedrijke word2vec-model wordt voorgesteld."
      ],
      "metadata": {
        "id": "jzJ9RL81IQv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basiswoord = \"tree\"\n",
        "l_andere_woorden = [\"leaf\", \"forest\", \"nature\", \"technology\"]\n",
        "\n",
        "for ander_woord in l_andere_woorden:\n",
        "  cos_sim = glove_vectoren.similarity(basiswoord, ander_woord)\n",
        "  print(f\"'{basiswoord}' versus '{ander_woord}': {round(float(cos_sim), 2)}\")"
      ],
      "metadata": {
        "id": "1x5IK90CF0MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gecontextualiseerde vectoren**\n",
        "Hoewel ze al een aardige stap in de goede richting zijn, hebben statische woordvectoren één groot probleem: ze voorzien slechts één enkele vector per woord, ongeacht of dat woord meerdere betekenissen heeft of niet (vandaar dus ook de term \"statisch\"). Met andere woorden, door de ogen van de computer zou de vector voor het woord *kiwi* er zomaar eens kunnen uitzien als de onvergetelijke verschijning in Figuur 1, aangezien *kiwi* als fruitsoort en *kiwi* als loopvogel in een statisch model gecondenseerd worden in één enkele numerieke waarde.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1blkyQRZbqwifOfbwoD6_KovGbxfIhqBT)\n",
        "\n",
        "*Figuur 1: Een allesomvattende Nieuw-Zeelandse kiwi.*\n"
      ],
      "metadata": {
        "id": "FCXvFwbx3EXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Om dit probleem te verhelpen, hebben we dus een model nodig dat **gecontextualiseerde** woordvectoren kan genereren. Als we terugkeren naar het voorbeeld van de kiwi, dan zou dit soort model in staat moeten kunnen zijn twee verschillende numerieke waardes te geven voor *kiwi* in volgende twee zinnen:\n",
        "- Een **kiwi** kan niet vliegen en bouwt daarom ook geen nest in een boom.\n",
        "- Sommige mensen vinden de gele **kiwi** lekkerder dan de groene, omdat ze iets zoeter van smaak zijn.\n",
        "\n",
        "Een ander goed voorbeeld is het woord *pad*, dat zowel kan verwijzen naar een amfibie als naar een smalle weg:\n",
        "- Het kronkelende **pad** door het bos bracht ons naar prachtige uitzichtpunten.\n",
        "- Wat moet ik doen als ik een **pad**, kikker of salamander vind?\n",
        "\n",
        "Het goede nieuws is dat dit type taalmodel al bestaat, zelfs voor een kleinere taal als het Nederlands. In onderstaande codecel laden we zo'n model in (via de variabele `model`), waarna we het gebruiken om bovenstaande zinnen te \"desambigueren\" (`l_zinnen_kiwi` en `l_zinnen_pad`). Hiervoor geven we telkens de volledige zin aan het model, want het is op basis van de context van de zin dat het model de (lichtjes andere) cijferwaardes voor de vectoren van de ambigue woorden zal bepalen (vandaar dus de term \"gecontextualiseerd\"). De uiteindelijke vectoren slaan we op in de variabelen `d_embeddings_kiwi` en `d_embeddings_kiwi`, waarna we opnieuw de PCA-techniek toepassen om de dimensies van de vectoren te reduceren naar twee.\n",
        "\n",
        "Het resultaat plotten we in een tweedimensionale grafiek, die ons toelaat om visueel vast te stellen dat we zowel voor *pad* als voor *kiwi* telkens verschillende numerieke waardes krijgen. Voor *pad* is het verschil heel duidelijk, voor *kiwi* blijken de twee verschillende betekenissen iets dichter bij elkaar te liggen. Desalniettemin kunnen we concluderen dat gecontextualiseerde woordvectoren de computer in staat stellen om het leggen van semantische verbanden gevoelig te verfijnen, aangezien hij nu op het niveau van woord*betekenissen* kan werken in plaats van op het niveau van woord*vormen*."
      ],
      "metadata": {
        "id": "umokAmJcxqdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WEETJE (1)**: Modellen die gecontextualiseerde woordvectoren genereren, worden vaak *large language models* of LLM's genoemd. Het is ook dit soort model dat aan de basis ligt van generatieve taalmodellen zoals die achter ChatGPT van OpenAI en Gemini van Google."
      ],
      "metadata": {
        "id": "psC-ceL5IHB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WEETJE (2)**: Het taalkundige fenomeen waarbij woorden meer dan één betekenis hebben, wordt \"homonymie\" (ongerelateerde betekenissen) of \"polysemie\" (verwante betekenissen) genoemd. Meer info kan je lezen op de site van [Taaladvies](https://www.vlaanderen.be/team-taaladvies/taaladviezen/homonymie-en-polysemie-taalkundige-termen)."
      ],
      "metadata": {
        "id": "aA3a2V6RIG4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TIP**: Voor wie op zoek is naar meer gespecialiseerde info rond gecontextualiseerde woordvectoren zijn de papers van [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) en [Devlin et al. (2019)](https://aclanthology.org/N19-1423/) twee absolute must reads. En voor wie het zo toegankelijk mogelijk mag blijven, is het artikel \"[Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/)\" van de Financial Times een aanrader."
      ],
      "metadata": {
        "id": "QomIsOzVIGvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OPMERKING**: Het laden van `model` zal een reeks automatische boodschappen met zich meebrengen. Deze mag je met een gerust hart negeren."
      ],
      "metadata": {
        "id": "7YCNY0weIGUX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MDOwAX_l-tg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "\n",
        "# (Nederlands) model laden\n",
        "naam_model = \"GroNLP/bert-base-dutch-cased\"\n",
        "tokeniser = AutoTokenizer.from_pretrained(naam_model)\n",
        "model = AutoModel.from_pretrained(naam_model, output_hidden_states=True)\n",
        "\n",
        "# zinnen definiëren (met woorden en leestekens gescheiden door een spatie)\n",
        "kiwi_zin_1 = \"Een kiwi kan niet vliegen en bouwt daarom ook geen nest in een boom .\"\n",
        "kiwi_zin_2 = \"Sommige mensen vinden de gele kiwi lekkerder dan de groene , omdat ze iets zoeter van smaak zijn .\"\n",
        "l_zinnen_kiwi = [kiwi_zin_1, kiwi_zin_2]\n",
        "\n",
        "pad_zin_1 = \"Het kronkelende pad door het bos bracht ons naar prachtige uitzichtpunten .\"\n",
        "pad_zin_2 = \"Wat moet ik doen als ik een pad , kikker of salamander vind ?\"\n",
        "l_zinnen_pad = [pad_zin_1, pad_zin_2]\n",
        "\n",
        "# gecontextualiseerde vectoren berekenen\n",
        "def gecontextualiseerde_woorden_berekenen(tokeniser, model, l_zinnen: list, woord: str) -> dict:\n",
        "  d_embeddings = {}\n",
        "\n",
        "  for idx, zin in enumerate(l_zinnen):\n",
        "    id_zin = idx + 1\n",
        "    idx_woord_in_zin = zin.split().index(woord)\n",
        "\n",
        "    inputs = tokeniser(zin.split(), is_split_into_words=True, return_tensors=\"pt\")\n",
        "    token_ids_word = np.where(np.array(inputs.word_ids()) == idx_woord_in_zin)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "    states = outputs.hidden_states\n",
        "    output = torch.stack([states[layer] for layer in [-4, -3, -2, -1]]).sum(0).squeeze()\n",
        "    word_tokens_output = output[token_ids_word]\n",
        "    d_embeddings[id_zin] = {\"embedding\": word_tokens_output.mean(dim=0), \"label\": f\"{woord}_zin_{id_zin}\"}\n",
        "\n",
        "  return d_embeddings\n",
        "\n",
        "d_embeddings_kiwi = gecontextualiseerde_woorden_berekenen(tokeniser, model, l_zinnen_kiwi, \"kiwi\")\n",
        "d_embeddings_pad = gecontextualiseerde_woorden_berekenen(tokeniser, model, l_zinnen_pad, \"pad\")\n",
        "\n",
        "# vectoren reduceren naar twee dimensies en plotten in grafiek\n",
        "l_vectoren = []\n",
        "l_labels = []\n",
        "\n",
        "for item in [d_embeddings_kiwi, d_embeddings_pad]:\n",
        "\n",
        "  for zin in item:\n",
        "    l_vectoren.append(item[zin][\"embedding\"])\n",
        "    l_labels.append(item[zin][\"label\"])\n",
        "\n",
        "l_vectoren_gereduceerd = PCA(n_components=2).fit_transform(l_vectoren)\n",
        "df_vectoren = pd.DataFrame(l_vectoren_gereduceerd)\n",
        "woorden_plotten_in_2d(df_vectoren, l_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dankzij de gecontextualiseerde woordvectoren met hun andere numerieke waardes kunnen we nu ook berekenen hoe gelijkaardig/verschillend twee verschillende gebruiken van een ambigu woord zijn. Laten we opnieuw de voorbeeldzinnen met *kiwi* en *pad* erbij nemen ter illustratie:\n",
        "- `kiwi_zin_1`: Een **kiwi** kan niet vliegen en bouwt daarom ook geen nest in een boom. (= *kiwi* als loopvogel)\n",
        "- `kiwi_zin_2`: Sommige mensen vinden de gele **kiwi** lekkerder dan de groene, omdat ze iets zoeter van smaak zijn. (= *kiwi* als fruitsoort)\n",
        "\n",
        "\n",
        "- `pad_zin_1`: Het kronkelende **pad** door het bos bracht ons naar prachtige uitzichtpunten. (= *pad* als smalle weg)\n",
        "- `pad_zin_2`: Wat moet ik doen als ik een **pad**, kikker of salamander vind? (= *pad* als amfibie)\n",
        "\n",
        "In onderstaande codecel definiëren we voor zowel *kiwi* als *pad* een nieuwe zin, waarna we opnieuw de techniek van de cosine similarity gebruiken om de mate van gelijkenis tussen (de gecontextualiseerde woordvector van) *kiwi* / *pad* in de twee voorbeeldzinnen en (de gecontextualiseerde woordvector van) *kiwi* / *pad* in de nieuwe zin te berekenen. Druk op de \"play\"-knop van de cel om de resultaten te zien."
      ],
      "metadata": {
        "id": "CU4oRpfh39H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# extra zinnen definiëren (met woorden en leestekens gescheiden door een spatie)\n",
        "kiwi_zin_3 = \"De kiwi is het symbool van Nieuw-Zeeland en dankt zijn naam aan het geluid dat het mannetje maakt .\"\n",
        "l_zinnen_kiwi = [kiwi_zin_1, kiwi_zin_2, kiwi_zin_3]\n",
        "\n",
        "pad_zin_3 = \"De gewone pad gebruikt voor het vangen van zijn prooi hoofdzakelijk de uitklapbare tong .\"\n",
        "l_zinnen_pad = [pad_zin_1, pad_zin_2, pad_zin_3]\n",
        "\n",
        "# gecontextualiseerde vectoren berekenen\n",
        "d_embeddings_kiwi = gecontextualiseerde_woorden_berekenen(tokeniser, model, l_zinnen_kiwi, \"kiwi\")\n",
        "d_embeddings_pad = gecontextualiseerde_woorden_berekenen(tokeniser, model, l_zinnen_pad, \"pad\")\n",
        "\n",
        "# cosine similarity berekenen\n",
        "cos_sim_formule = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "for item in [d_embeddings_kiwi, d_embeddings_pad]:\n",
        "\n",
        "  for vergelijking in [(1, 2), (1, 3), (2, 3)]:\n",
        "    id_zin_1 = vergelijking[0]\n",
        "    id_zin_2 = vergelijking[1]\n",
        "    cos_sim = cos_sim_formule(item[id_zin_1][\"embedding\"], item[id_zin_2][\"embedding\"])\n",
        "    print(f\"{item[id_zin_1]['label']} versus {item[id_zin_2]['label']}: {round(float(cos_sim), 2)}\")\n",
        "\n",
        "  print(\"\\n-----------\\n\")\n"
      ],
      "metadata": {
        "id": "o1KOGbCe39Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusie\n",
        "Wanneer wij als mens de computer letters, woorden, zinnen, of teksten laten verwerken, vragen we de computer eigenlijk dat hij een betekenisloze reeks nulletjes en eentjes (oftewel *bytes*) analyseert. In deze notebook hebben we aangetoond hoe de computer woorden en betekenissen kan herkennen in deze zee van *bytes*. Door de computer een grote verzameling teksten te geven (bv. Wikipediapagina's of Twitter/X-data), kunnen we een model trainen dat 1) bijhoudt welke *byte*-reeksen (woorden, dus) vaak in elkaars buurt voorkomen en 2) een gelijkaardige cijferwaarde toekent aan deze reeksen (in de vorm van een zogenaamde \"vector\"). Zoals geïllustreerd in deze notebook stellen deze **statische woordvectoren** de computer in staat om betekenisvolle verbanden te leggen tussen woorden en de contexten waarin ze voorkomen. Met wat gezonde zin voor overdrijving zouden we zelfs kunnen argumenteren dat ze een soort van \"wereldkennis\" bevatten.\n",
        "\n",
        "Statische woordvectoren hebben echter één groot probleem: ze zijn gelinkt aan de *woordvorm*, en kunnen dus - bij woorden met meer dan één betekenis - geen onderscheid maken tussen deze verschillende betekenissen. Aangezien homonymie en polysemie wijdverspreide fenomenen zijn, vormt dit een serieuze beperking het taalbegrip van een computer. Gelukkig kon een nieuwe generatie taalmodellen hiervoor een (deel van de) oplossing aanreiken. Deze modellen nemen de volledige zin (of een bredere context) als input en passen de waardes van de woordvectoren aan op basis van de woorden die naast het ambigue item voorkomen (waardoor we **gecontextualiseerde woordvectoren** verkrijgen). Als eindresultaat hebben we dus een computer die in staat zou moeten zijn om woorden te herkennen én de semantiek rond die woorden te vatten, zelfs als het gaat om homonieme of polyseme woorden."
      ],
      "metadata": {
        "id": "weCsu3ebuBiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bronnen**\n",
        "- Firth, J.R. (1957). \"A synopsis of linguistic theory 1930-55\", in *Selected papers of J.R. Firth 1952-1959*. London: Longman, 168-205."
      ],
      "metadata": {
        "id": "HFo3w_5vh0XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Credits**\n",
        "- **Auteurs**: Jasper Degraeuwe and Loic De Langhe\n",
        "- **Licentie**: [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1g1vHFB7UJhnVOPg0EVirQcxAKndVnprP\" width=\"200\" height=\"160\">\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "bTjYGgnDyei5"
      }
    }
  ]
}